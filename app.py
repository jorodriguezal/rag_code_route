# app.py
import os
import streamlit as st
from dotenv import load_dotenv

from llama_index.core import Settings
from llama_index.core.query_engine import RetrieverQueryEngine
from llama_index.core.retrievers import VectorIndexRetriever
from llama_index.core.postprocessor import SimilarityPostprocessor

from src.indexer import build_or_load_index, reset_index
from src.prompts import QA_SYSTEM, SUMMARY_SYSTEM


load_dotenv()

st.set_page_config(page_title="RAG Code de la route", page_icon="üöó", layout="wide")
st.title(" Assistant RAG ‚Äì Code de la route (PDF)")

# --- v√©rification env ---
needed = [
    "AZURE_OPENAI_ENDPOINT",
    "AZURE_OPENAI_API_KEY",
    "AZURE_OPENAI_API_VERSION",
    "AZURE_OPENAI_CHAT_DEPLOYMENT",
    "AZURE_OPENAI_EMBEDDING_DEPLOYMENT",
    "AZURE_OPENAI_EMBEDDING_API_VERSION",
]
missing = [k for k in needed if not os.getenv(k)]
if missing:
    st.error("Variables manquantes dans .env : " + ", ".join(missing))
    st.stop()

# --- cache index ---
@st.cache_resource
def get_index():
    return build_or_load_index()

with st.sidebar:
    st.subheader(" Index")
    if st.button(" R√©-indexer (reset Chroma)"):
        st.cache_resource.clear()
        reset_index()
        st.success("Index supprim√©. Relance l‚Äôapp ou clique sur une action.")
        st.stop()

    top_k = st.slider("Nombre de passages r√©cup√©r√©s (k)", 2, 10, 5)
    st.caption("Mets ton PDF dans `data/` puis pose des questions.")

tab1, tab2 = st.tabs([" Questions / R√©ponses", " R√©sum√©"])

# =========================
# TAB 1 : Q&A RAG
# =========================
with tab1:
    st.subheader(" Pose une question sur le Code de la route")

    query = st.text_input(
        "Question",
        placeholder="Ex: Que signifie un panneau triangulaire ?",
    )

    col1, col2 = st.columns([1, 1])
    with col1:
        ask_btn = st.button("R√©pondre", use_container_width=True)
    with col2:
        show_sources = st.toggle("Afficher les sources d√©taill√©es", value=True)

    if ask_btn:
        if not query.strip():
            st.warning("√âcris une question d‚Äôabord.")
            st.stop()

        index = get_index()

        # Retriever + Query Engine
        retriever = VectorIndexRetriever(index=index, similarity_top_k=top_k)
        query_engine = RetrieverQueryEngine(
            retriever=retriever,
            node_postprocessors=[SimilarityPostprocessor(similarity_cutoff=0.2)],
        )

        # On ajoute le "system" en pr√©fixe au prompt via Settings.llm
        # (LlamaIndex: on peut injecter dans le query)
        full_query = f"{QA_SYSTEM}\n\nQuestion: {query}"

        with st.spinner("Recherche dans le PDF + g√©n√©ration..."):
            response = query_engine.query(full_query)

        st.markdown("###  R√©ponse")
        st.write(str(response))

        # Sources (si disponibles)
        if show_sources:
            st.markdown("###  Sources")
            try:
                nodes = response.source_nodes or []
            except Exception:
                nodes = []

            if not nodes:
                st.info("Aucune source affichable (selon la r√©ponse).")
            else:
                for i, n in enumerate(nodes, 1):
                    meta = n.node.metadata or {}
                    file_name = meta.get("file_name") or meta.get("filename") or meta.get("source") or "PDF"
                    page = meta.get("page_label") or meta.get("page") or "?"
                    score = getattr(n, "score", None)

                    st.write(f"**[{i}]** `{file_name}` ‚Äî page **{page}**" + (f" ‚Äî score {score:.3f}" if score else ""))
                    excerpt = n.node.get_text()[:700].replace("\n", " ")
                    st.caption(excerpt + ("..." if len(n.node.get_text()) > 700 else ""))

# =========================
# TAB 2 : SUMMARY
# =========================
with tab2:
    st.subheader(" R√©sum√© (global / points cl√©s)")

    mode = st.selectbox(
        "Type de r√©sum√©",
        ["R√©sum√© court", "Points cl√©s (bullet points)", "Fiche de r√©vision simple"],
        index=1,
    )

    run_sum = st.button("G√©n√©rer le r√©sum√©", use_container_width=True)

    if run_sum:
        index = get_index()

        # Pour un r√©sum√© global, on r√©cup√®re des passages "repr√©sentatifs"
        # Astuce simple: on fait plusieurs requ√™tes internes puis on r√©sume.
        # (Simple, acad√©mique, et marche bien.)
        seed_queries = [
            "r√©sume les r√®gles importantes",
            "signalisation panneaux marquage",
            "priorit√©s intersections",
            "vitesse distance s√©curit√©",
            "sanctions alcool t√©l√©phone",
        ]

        retriever = VectorIndexRetriever(index=index, similarity_top_k=6)
        collected = []
        for q in seed_queries:
            nodes = retriever.retrieve(q)
            for n in nodes:
                txt = n.node.get_text()
                if txt and txt not in collected:
                    collected.append(txt)

        # Limiter la taille envoy√©e au mod√®le
        joined = "\n\n".join(collected[:25])

        if mode == "R√©sum√© court":
            instr = "Fais un r√©sum√© court (8 √† 12 lignes) en fran√ßais."
        elif mode == "Fiche de r√©vision simple":
            instr = (
                "Cr√©e une fiche de r√©vision simple: "
                "1) D√©finitions, 2) R√®gles cl√©s, 3) Panneaux/Signalisation (si pr√©sent), "
                "4) Sanctions (si pr√©sent)."
            )
        else:
            instr = "Donne uniquement les points cl√©s sous forme de puces (15 √† 25 puces max)."

        prompt = f"""{SUMMARY_SYSTEM}

Instruction: {instr}

Texte √† r√©sumer (extraits du PDF):
{joined}

Sortie attendue:
- en fran√ßais
- clair et structur√©
"""

        with st.spinner("G√©n√©ration du r√©sum√©..."):
            # Utiliser directement le LLM configur√© dans Settings (d√©j√† set dans indexer)
            llm = Settings.llm
            resp = llm.complete(prompt)

        st.markdown("###  R√©sum√©")
        st.write(resp.text)
